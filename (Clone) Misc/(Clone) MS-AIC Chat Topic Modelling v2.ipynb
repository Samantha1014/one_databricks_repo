{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b348997-fa41-4beb-a7b1-135d691e28b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# AIC Chat Topic Modelling\n",
    "\n",
    "Purpose: To identify topics based off AIC chat transcripts, and then classify conversations by topic.\n",
    "\n",
    "Compute Cluster: fs-cluster-m\n",
    "\n",
    "Created: 1/5/2024\n",
    "\n",
    "Current status: In progress (16/5/2025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdb76df6-f0e3-4aa7-9ce6-eb0d79f57dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s001 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755f9ebc-212b-47d9-a919-212eeac50fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "%pip install bertopic==0.16.0 \\\n",
    "  transformers==4.34.0 \\\n",
    "  sentence-transformers==3.0.0 \\\n",
    "  datasets==2.14.5 \\\n",
    "  huggingface-hub==0.17.3 \\\n",
    "  einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ddb504-503a-426d-9ddf-645d21fd4af5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from hdbscan import HDBSCAN\n",
    "from datasets import Dataset\n",
    "from bertopic import BERTopic\n",
    "from pyspark.sql.functions import lit\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['SENTENCE_TRANSFORMERS_OFFLINE']='1'\n",
    "os.environ['DATASETS_OFFLINE']='1'\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2a669ba-f68c-4472-b051-b1f60047efec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s002 Snowflake Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51586888-8cc9-4989-8fc7-ba9c1d68c080",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the private key from Azure Key Vault via Databricks secrets\n",
    "password = dbutils.secrets.get(scope=\"auea-kv-sbx-dxdtlprdct01\", key=\"sfdbrsdskey\")\n",
    "\n",
    "# Define Snowflake connection options\n",
    "sf_options = {\n",
    "    \"sfURL\": \"vodafonenz_prod.australia-east.azure.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"SVC_LAB_DS_DATABRICKS\",\n",
    "    \"pem_private_key\": password.replace('\\\\n', '\\n'),\n",
    "    \"sfDatabase\": \"LAB_ML_STORE\",\n",
    "    \"sfSchema\": \"SANDBOX\",\n",
    "    \"sfWarehouse\": \"LAB_DS_WH_SCALE\"\n",
    "}\n",
    "\n",
    "# Load the Snowflake table into a Spark DataFrame\n",
    "raw_df = spark.read \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sf_options) \\\n",
    "    .option(\"dbtable\", \"AIC_CHAT_CONTENT_20250611\") \\\n",
    "    .load()\n",
    "\n",
    "raw_pd_df = raw_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2febdf-58b7-408f-bf2d-2c80231d22c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    raw_df\n",
    "    .limit(100)      \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f050cb-4634-4019-a907-40545078843a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s003 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d6ca07c-23e1-4df9-adcc-333f9f759d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keep relevant columns\n",
    "df2 = raw_pd_df[['CONTACTID', 'CONTENT', 'INPUTID', 'ROLE', 'AGT_AFTERCONTACTWORKENDTIMESTAMP']]\n",
    "\n",
    "# Remove rows starting with '(concierge)'\n",
    "df2 = df2[~df2['CONTENT'].str.startswith('(concierge)')]\n",
    "\n",
    "# Remove prefixes from content\n",
    "df2['CONTENT'] = df2['CONTENT'].str.replace(r'^\\([^\\)]+\\)\\s*', '', regex=True)\n",
    "\n",
    "# Add formatted content column with '[a]', '[u]' prefixes\n",
    "df2['FORMATTED_CONTENT'] = df2.apply(lambda row: f\"[u]{row['CONTENT']}\" if row['ROLE'] == 'user' else f\"[a]{row['CONTENT']}\", axis=1)\n",
    "\n",
    "# Clean 'FORMATTED_CONTENT' column to remove prefixes like (rag)\n",
    "df2['FORMATTED_CONTENT'] = df2['FORMATTED_CONTENT'].str.replace(r'\\([^\\)]+\\)\\s*', '', regex=True)\n",
    "\n",
    "# Split into user and non-user dataframes and drop duplicate inputids\n",
    "user_df = df2[df2['ROLE'] == 'user'].drop_duplicates(subset='INPUTID', keep='first')\n",
    "non_user_df = df2[df2['ROLE'] != 'user']\n",
    "\n",
    "df2 = pd.concat([user_df, non_user_df]).sort_index()\n",
    "\n",
    "# Drop rows from dataframe where 'CONTENT' is null or NA\n",
    "df2 = df2.dropna(subset=['CONTENT'])\n",
    "df2 = df2[~df2['CONTENT'].isin(['NA'])]\n",
    "\n",
    "# Sort so that user text is before assistant text\n",
    "df2 = df2.sort_values(by=['CONTACTID', 'AGT_AFTERCONTACTWORKENDTIMESTAMP', 'ROLE'],\n",
    "                      ascending=[True, True, False])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df2 = df2.drop(columns=['INPUTID', 'AGT_AFTERCONTACTWORKENDTIMESTAMP'])\n",
    "\n",
    "# Group full and user content by contactid\n",
    "content_by_contactid = df2.groupby('CONTACTID')['CONTENT'].apply(lambda x: ' '.join(x)).reset_index(name='ALL_CONTENT')\n",
    "user_content_by_contactid = df2[df2['ROLE'] == 'user'].groupby('CONTACTID')['CONTENT'].apply(lambda x: ' '.join(x)).reset_index(name='USER_CONTENT')\n",
    "\n",
    "# Group formatted content by contactid\n",
    "formatted_content_by_contactid = df2.groupby('CONTACTID')['FORMATTED_CONTENT'].apply(lambda x: ' '.join(x)).reset_index(name='FORMATTED_CONTENT')\n",
    "\n",
    "# Merge outputs\n",
    "merged_df = pd.merge(content_by_contactid, user_content_by_contactid, on='CONTACTID', how='left')\n",
    "merged_df = pd.merge(merged_df, formatted_content_by_contactid, on='CONTACTID', how='left')\n",
    "\n",
    "# Show dataframe\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea439406-e2a2-4a9b-b287-792dda815e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s004 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e972bfb-acd2-4163-aeb5-17f456f4e02a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load local mpnet model\n",
    "checkpoint = '/dbfs/FileStore/tables/ms/saved_mpnet_model/'\n",
    "embedding_model = SentenceTransformer(checkpoint, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970bbdce-e59e-4a86-923d-0339e0598889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert user-only text and all text conversations to lists\n",
    "user_documents_cleaned = merged_df['USER_CONTENT'].tolist()\n",
    "all_documents_cleaned = merged_df['ALL_CONTENT'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a062e0-da21-4bd9-9166-f0f642d94755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "user_embeddings = embedding_model.encode(user_documents_cleaned, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62b89d3-5808-4d37-a10c-e8f8919b813f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s005 Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f110cced-c151-4a27-b402-210449fc449e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Level 1 topics\n",
    "\n",
    "# For dimensionality reduction\n",
    "low_umap_model = umap.UMAP(\n",
    "    n_components=3,\n",
    "    n_neighbors=10,\n",
    "    min_dist = 0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# For clustering embeddings\n",
    "low_hdbscan_model = HDBSCAN(\n",
    "    min_samples=2,\n",
    "    min_cluster_size=8,\n",
    "    cluster_selection_method = 'eom',\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "# Set-up vectoriser model to remove stopwords after topics have been generated\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Reduce redundancy in topic representations\n",
    "representation_model = MaximalMarginalRelevance(diversity=0.2)\n",
    "\n",
    "# Create Level 1 topic model\n",
    "low_topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=low_umap_model,\n",
    "    hdbscan_model=low_hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6695281-73cc-4404-a282-d9de317a0dd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print level 1 topics\n",
    "\n",
    "low_topics, low_probs = low_topic_model.fit_transform(user_documents_cleaned, user_embeddings)\n",
    "\n",
    "low_topic_model.get_topic_info()\n",
    "\n",
    "for topic in range(len(low_topic_model.get_topics())):\n",
    "    print(f\"Topic {topic}: {low_topic_model.get_topic(topic)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e8b3a7-e5d4-44d2-ad6b-64a5a4624550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def run_subtopic_model(docs, embeddings, level=2):\n",
    "    # Set parameters\n",
    "    umap_dims = {2: 5, 3: 3}\n",
    "    n_neighbors = {2: 10, 3: 7}\n",
    "    min_cluster_size = {2: 3, 3: 2}\n",
    "    diversity = {2: 0.2, 3: 0.3}\n",
    "\n",
    "    # Setup model\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap.UMAP(\n",
    "            n_components=umap_dims[level],\n",
    "            n_neighbors=n_neighbors[level],\n",
    "            min_dist=0.5,\n",
    "            metric='cosine',\n",
    "            random_state=42\n",
    "        ),\n",
    "        hdbscan_model=HDBSCAN(\n",
    "            min_samples=2,\n",
    "            min_cluster_size=min_cluster_size[level],\n",
    "            cluster_selection_method='eom',\n",
    "            prediction_data=True\n",
    "        ),\n",
    "        vectorizer_model=CountVectorizer(stop_words='english'),\n",
    "        representation_model=MaximalMarginalRelevance(diversity=diversity[level]),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    topics, _ = topic_model.fit_transform(docs, embeddings)\n",
    "    topic_counts = Counter(topics)\n",
    "\n",
    "    for topic_num in topic_model.get_topic_info().Topic:\n",
    "        if topic_num == -1:\n",
    "            continue  # Skip other topic\n",
    "\n",
    "        topic_docs_idx = [i for i, t in enumerate(topics) if t == topic_num]\n",
    "        topic_docs = [docs[i] for i in topic_docs_idx]\n",
    "        topic_embeddings = np.array([embeddings[i] for i in topic_docs_idx])\n",
    "\n",
    "        indent = \"  \" * (level - 1)\n",
    "        print(f\"{indent}Sub{'-sub' * (level - 2)}topic {topic_num} ({len(topic_docs)} documents):\")\n",
    "        print(f\"{indent}  Keywords: {topic_model.get_topic(topic_num)}\\n\")\n",
    "\n",
    "        # Recursive subtopic modeling\n",
    "        if level < 3 and len(topic_docs) >= 5:\n",
    "            run_subtopic_model(topic_docs, topic_embeddings, level + 1)\n",
    "\n",
    "\n",
    "for topic_num in low_topic_model.get_topic_info().Topic:\n",
    "    if topic_num == -1:\n",
    "        continue  # Skip other topic\n",
    "\n",
    "    # Get docs and embeddings for this topic\n",
    "    docs_idx = [i for i, t in enumerate(low_topics) if t == topic_num]\n",
    "    docs_subset = [user_documents_cleaned[i] for i in docs_idx]\n",
    "    embeddings_subset = np.array([user_embeddings[i] for i in docs_idx])\n",
    "\n",
    "    print(f\"\\n=== Main Topic {topic_num} ({len(docs_subset)} documents) ===\")\n",
    "    print(f\"Keywords: {low_topic_model.get_topic(topic_num)}\\n\")\n",
    "\n",
    "    if len(docs_subset) < 5:\n",
    "        print(f\"Skipping topic {topic_num}, too few docs.\\n\")\n",
    "        continue\n",
    "\n",
    "    # Start subtopic modeling from level 2\n",
    "    run_subtopic_model(docs_subset, embeddings_subset, level=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a5acae2-3c5f-4f78-9113-22b642b6e944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s006 Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11614279-93e5-48dd-aa44-77353af449a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define topics\n",
    "\n",
    "level_1_topics = [\n",
    "    \"Connectivity and Technical Support\",\n",
    "    \"Roaming and International\",\n",
    "    \"Billing and Payments\",\n",
    "    \"Account Management and Balance\",\n",
    "    \"Other\",\n",
    "    \"Device Purchases and Inquiries\",\n",
    "    \"Plans and Services\"\n",
    "]\n",
    "\n",
    "level_2_topics = [\n",
    "    \"Network & Service Connectivity\",\n",
    "    \"Roaming Details and Inquiries\",\n",
    "    \"Payment Processing Issues\",\n",
    "    \"Billing and Invoice Queries\",\n",
    "    \"Account Changes\",\n",
    "    \"Other\",\n",
    "    \"Account Balance and Top-ups\",\n",
    "    \"Device Buying & Upgrading\",\n",
    "    \"Account Access\",\n",
    "    \"Plan Inquiries and Information\",\n",
    "    \"Plan Changes and Switching\",\n",
    "    \"Roaming Technical Issues and Support\",\n",
    "    \"Account & App Access Problems\",\n",
    "    \"eSIM and Device setup\",\n",
    "    \"Billing Disputes\"\n",
    "]\n",
    "\n",
    "\n",
    "level_3_topics = [\n",
    "    \"Messaging Issues\",\n",
    "    \"Roaming Charges and Add-ons\",\n",
    "    \"Top-Up and Balance Issues\",\n",
    "    \"Roaming Usage and Coverage\",\n",
    "    \"Network Registration and Service Access Issues\",\n",
    "    \"Bill Amount and Payment Issues\",\n",
    "    \"Plan or Account Modifications\",\n",
    "    \"Other\",\n",
    "    \"Top-up Not Applied\",\n",
    "    \"Device Purchase and Availability\",\n",
    "    \"Unexpected Balance Change\",\n",
    "    \"PIN & Security Code Recovery\",\n",
    "    \"Bill Access and Documentation\",\n",
    "    \"Plan Pricing and Details\",\n",
    "    \"Prepaid/Monthly Plan Changes\",\n",
    "    \"Roaming Technical Support and Troubleshooting\",\n",
    "    \"Social Media App Access Problems\",\n",
    "    \"eSIM Activation and Transfer\",\n",
    "    \"Plan Usage and Eligibility\",\n",
    "    \"Incorrect Charges and Billing Errors\",\n",
    "    \"Duplicate or Missing Charges\",\n",
    "    \"Address or Number Update\",\n",
    "    \"eSIM Purchase and Setup\",\n",
    "    \"Refunds and Credits Issues\",\n",
    "    \"Broadband/WiFi and Other Add-ons\",\n",
    "    \"Upgrades and Trade-ins\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847e3313-6e6a-44fd-b808-203109d38fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create models\n",
    "\n",
    "def create_bertopic_model(embedding_model, topic_list, zeroshot_min_similarity=0.1, top_n_words=10, verbose=True):\n",
    "    return BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        zeroshot_min_similarity=zeroshot_min_similarity,\n",
    "        zeroshot_topic_list=topic_list,\n",
    "        top_n_words=top_n_words,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "level_1_model = create_bertopic_model(embedding_model, level_1_topics)\n",
    "level_2_model = create_bertopic_model(embedding_model, level_2_topics)\n",
    "level_3_model = create_bertopic_model(embedding_model, level_3_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb47e4b3-ed9a-48de-b2b4-5225ed3b6c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def encode_texts(embedding_model, texts, show_progress_bar=True):\n",
    "    return embedding_model.encode(texts, show_progress_bar=show_progress_bar)\n",
    "\n",
    "all_documents_embeddings = encode_texts(embedding_model, all_documents_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5caf8904-8ec1-484d-9d9e-e06489b513b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply topic classification\n",
    "\n",
    "def fit_transform_model(model, documents, embeddings):\n",
    "    return model.fit_transform(documents, embeddings)\n",
    "\n",
    "level_1_topics_all, _ = fit_transform_model(level_1_model, all_documents_cleaned, all_documents_embeddings)\n",
    "level_2_topics_all, _ = fit_transform_model(level_2_model, all_documents_cleaned, all_documents_embeddings)\n",
    "level_3_topics_all, _ = fit_transform_model(level_3_model, all_documents_cleaned, all_documents_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b448fa6e-af95-43a1-8adc-3ec849505c83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "level1_info_df = level_1_model.get_topic_info()\n",
    "level2_info_df = level_2_model.get_topic_info()\n",
    "level3_info_df = level_3_model.get_topic_info()\n",
    "\n",
    "# Set-up topic name mappings\n",
    "level1_topic_map = dict(zip(level1_info_df['Topic'], level1_info_df['Name']))\n",
    "level2_topic_map = dict(zip(level2_info_df['Topic'], level2_info_df['Name']))\n",
    "level3_topic_map = dict(zip(level3_info_df['Topic'], level3_info_df['Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e565c267-c5b8-4d44-bafd-0a0174c77bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get level 1,2,3 topics and confidence scores\n",
    "\n",
    "level_1_topics1, level_1_scores = level_1_model.transform(all_documents_cleaned)\n",
    "level_2_topics2, level_2_scores = level_2_model.transform(all_documents_cleaned)\n",
    "level_3_topics3, level_3_scores = level_3_model.transform(all_documents_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d95838-0d2b-42e0-85a2-37096e1bcf37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s007 Evaluate and export zero-shot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "281a46a8-e0c9-4137-a467-167a56859eeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export CSV\n",
    "\n",
    "def map_topic_names(topic_numbers, topic_map):\n",
    "    return [topic_map.get(t, 'No Topic') for t in topic_numbers]\n",
    "\n",
    "df_final = pd.DataFrame({\n",
    "    'CONTACTID': merged_df['CONTACTID'],\n",
    "    'CONTENT': merged_df['FORMATTED_CONTENT'],\n",
    "\n",
    "    'LEVEL1_TOPIC_NUMBER': level_1_topics1,\n",
    "    'LEVEL1_CONFIDENCE_SCORE': level_1_scores,\n",
    "    'LEVEL1_TOPIC_NAME': map_topic_names(level_1_topics1, level1_topic_map),\n",
    "\n",
    "    'LEVEL2_TOPIC_NUMBER': level_2_topics2,\n",
    "    'LEVEL2_CONFIDENCE_SCORE': level_2_scores,\n",
    "    'LEVEL2_TOPIC_NAME': map_topic_names(level_2_topics2, level2_topic_map),\n",
    "\n",
    "    'LEVEL3_TOPIC_NUMBER': level_3_topics3,\n",
    "    'LEVEL3_CONFIDENCE_SCORE': level_3_scores,\n",
    "    'LEVEL3_TOPIC_NAME': map_topic_names(level_3_topics3, level3_topic_map)\n",
    "})\n",
    "\n",
    "df_final.to_csv(\"topic_assignment_export8.csv\", index=False)\n",
    "print(\"Saved final CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434ea345-269b-4764-b19d-52fc2b209efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the predicted and gold CSVs\n",
    "ml_df = pd.read_csv(\"topic_assignment_export7.csv\")\n",
    "human_df = pd.read_csv(\"/dbfs/FileStore/tables/ms/newnewbook3c.csv\")\n",
    "\n",
    "# Merge them on CONTACTID\n",
    "merged_df = pd.merge(ml_df, human_df, on=\"CONTACTID\")\n",
    "\n",
    "# Rename columns for better comparison\n",
    "merged_df = merged_df.rename(columns={\n",
    "    \"LEVEL1_TOPIC_NAME\": \"ML_L1\",\n",
    "    \"LEVEL2_TOPIC_NAME\": \"ML_L2\",\n",
    "    \"LEVEL3_TOPIC_NAME\": \"ML_L3\",\n",
    "    \"CLASSIFIED_L1\": \"HUMAN_L1\",\n",
    "    \"CLASSIFIED_L2\": \"HUMAN_L2\",\n",
    "    \"CLASSIFIED_L3\": \"HUMAN_L3\"\n",
    "})\n",
    "\n",
    "# Calculate accuracy for each level\n",
    "level_1_accuracy = (merged_df[\"ML_L1\"] == merged_df[\"HUMAN_L1\"]).mean()\n",
    "level_2_accuracy = (merged_df[\"ML_L2\"] == merged_df[\"HUMAN_L2\"]).mean()\n",
    "level_3_accuracy = (merged_df[\"ML_L3\"] == merged_df[\"HUMAN_L3\"]).mean()\n",
    "\n",
    "# Print results\n",
    "print(f\"Level 1 Accuracy: {level_1_accuracy:.2%}\")\n",
    "print(f\"Level 2 Accuracy: {level_2_accuracy:.2%}\")\n",
    "print(f\"Level 3 Accuracy: {level_3_accuracy:.2%}\")\n",
    "\n",
    "accuracy_scores = [level_1_accuracy, level_2_accuracy, level_3_accuracy]\n",
    "levels = ['Level 1', 'Level 2', 'Level 3']\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(levels, accuracy_scores, color=['#4CAF50', '#2196F3', '#FFC107'])\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy at Each Classification Level')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height + 0.02, f'{height:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c39394-1115-44e5-b117-93aa701d1c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate chart of accuracy at random for reference\n",
    "\n",
    "num_trials = 1000\n",
    "\n",
    "random_acc_level_1 = []\n",
    "random_acc_level_2 = []\n",
    "random_acc_level_3 = []\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    random_guess_l1 = np.random.choice(level_1_topics, size=len(merged_df))\n",
    "    random_guess_l2 = np.random.choice(level_2_topics, size=len(merged_df))\n",
    "    random_guess_l3 = np.random.choice(level_3_topics, size=len(merged_df))\n",
    "\n",
    "    acc_l1 = np.mean(random_guess_l1 == merged_df[\"HUMAN_L1\"])\n",
    "    acc_l2 = np.mean(random_guess_l2 == merged_df[\"HUMAN_L2\"])\n",
    "    acc_l3 = np.mean(random_guess_l3 == merged_df[\"HUMAN_L3\"])\n",
    "\n",
    "    random_acc_level_1.append(acc_l1)\n",
    "    random_acc_level_2.append(acc_l2)\n",
    "    random_acc_level_3.append(acc_l3)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.boxplot([random_acc_level_1, random_acc_level_2, random_acc_level_3], labels=['Level 1', 'Level 2', 'Level 3'])\n",
    "plt.ylabel('Random Guess Accuracy')\n",
    "plt.title('Random Baseline Accuracy Distribution by Level')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7ac4e62-2bc4-4c79-8658-808e5544cd68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upload results to Snowflake table\n",
    "df_spark = spark.createDataFrame(df_final)\n",
    "\n",
    "# Add the current timestamp as a new column\n",
    "df_spark = df_spark.withColumn(\"created_datetime\", lit(datetime.now()))\n",
    "\n",
    "# Now write to Snowflake\n",
    "df_spark.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sf_options) \\\n",
    "    .option(\"dbtable\", \"SANDBOX.AIC_ZEROSHOT_LABEL_LEVEL123\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "233e5882-3301-4387-bb6a-0792ab53ac49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s008 Implementing the supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0877a890-0e4f-422a-955a-335a4dfdfbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hierarchy dictionary\n",
    "hierarchy = {\n",
    "    \"Connectivity and Technical Support\": {\n",
    "        \"Network & Service Connectivity\": [\"Messaging Issues\", \"Network Registration and Service Access Issues\"],\n",
    "        \"Account & App Access Problems\": [\"Social Media App Access Problems\"],\n",
    "    },\n",
    "    \"Roaming and International\": {\n",
    "        \"Roaming Details and Inquiries\": [\"Roaming Charges and Add-ons\", \"Roaming Usage and Coverage\"],\n",
    "        \"Roaming Technical Issues and Support\": [\"Roaming Technical Support and Troubleshooting\"],\n",
    "    },\n",
    "    \"Billing and Payments\": {\n",
    "        \"Billing and Invoice Queries\": [\"Bill Amount and Payment Issues\", \"Bill Access and Documentation\"],\n",
    "        \"Billing Disputes\": [\"Incorrect Charges and Billing Errors\", \"Refunds and Credit Issues\"],\n",
    "        \"Payment Processing Issues\": [\"Top-Up and Balance Issues\", \"Duplicate or Missing Charges\"],\n",
    "    },\n",
    "    \"Account Management and Balance\": {\n",
    "        \"Account Access\": [\"PIN & Security Code Recovery\"],\n",
    "        \"Account Balance and Top-ups\": [\"Top-up Not Applied\", \"Unexpected Balance Change\"],\n",
    "        \"Account Changes\": [\"Plan or Account Modifications\", \"Address or Number Update\"],\n",
    "    },\n",
    "    \"Other\": {\n",
    "        \"Other\": [\"Other\"],\n",
    "    },\n",
    "    \"Device Purchases and Inquiries\": {\n",
    "        \"Device Buying & Upgrading\": [\"Device Purchase and Availability\", \"Upgrades and Trade-ins\"],\n",
    "        \"eSIM and Device setup\": [\"eSIM Activation and Transfer\", \"eSIM Purchase and Setup\"],\n",
    "    },\n",
    "    \"Plans and Services\": {\n",
    "        \"Plan Changes and Switching\": [\"Prepaid/Monthly Plan Changes\", \"Broadband/WiFi and Other Add-ons\"],\n",
    "        \"Plan Inquiries and Information\": [\"Plan Pricing and Details\", \"Plan Usage and Eligibility\"],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846df88e-7a62-4d54-8206-b7e5cbcdec37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract necessary components from the pipeline results\n",
    "y1_test_pred = models['level1'][1]  # Predicted labels for Level 1\n",
    "le1 = models['level1'][3]  # LabelEncoder for Level 1\n",
    "X_test = models['X_test']  # Test set features\n",
    "contact_ids = models['contact_ids']  # Contact IDs for test set\n",
    "\n",
    "# Ensure contact_ids is a pandas Series and aligned with X_test\n",
    "contact_ids = contact_ids.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "\n",
    "# Verify lengths match\n",
    "assert len(contact_ids) == len(X_test) == len(y1_test_pred), \"Mismatch in test set lengths\"\n",
    "\n",
    "# Create a DataFrame for the test set predictions\n",
    "test_df = pd.DataFrame({\n",
    "    'CONTACTID': contact_ids,\n",
    "    'USER_CONTENT': X_test,\n",
    "    'Predicted_Label_Enc': y1_test_pred,\n",
    "    'Predicted_Label': le1.inverse_transform(y1_test_pred),\n",
    "    'Confidence': models['confidence']['level1']\n",
    "})\n",
    "\n",
    "# Merge with gold_data_l23 to get the true CLASSIFIED_L1 labels based on CONTACTID\n",
    "test_df = test_df.merge(\n",
    "    gold_data_l23[['CONTACTID', 'CLASSIFIED_L1']],\n",
    "    on='CONTACTID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename CLASSIFIED_L1 to True_Label for clarity\n",
    "test_df = test_df.rename(columns={'CLASSIFIED_L1': 'True_Label'})\n",
    "\n",
    "# Create a mask for misclassified instances\n",
    "# Encode True_Label using le1 for comparison with Predicted_Label_Enc\n",
    "test_df['True_Label_Enc'] = le1.transform(test_df['True_Label'])\n",
    "misclassified_mask = test_df['Predicted_Label_Enc'] != test_df['True_Label_Enc']\n",
    "\n",
    "# Filter for misclassified instances\n",
    "misclassified_df = test_df[misclassified_mask][[\n",
    "    'CONTACTID', 'USER_CONTENT', 'True_Label', 'Predicted_Label', 'Confidence'\n",
    "]]\n",
    "\n",
    "# Reset index for clarity\n",
    "misclassified_df = misclassified_df.reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "misclassified_df.to_csv('mismissclassified1.csv', index=False)\n",
    "\n",
    "# Print the DataFrame to inspect\n",
    "print(\"Misclassified Instances for Level 1:\")\n",
    "print(misclassified_df)\n",
    "\n",
    "# Verify True_Label matches gold_data_l23\n",
    "print(\"\\nSample of True_Label values from misclassified_df:\")\n",
    "print(misclassified_df[['CONTACTID', 'True_Label']].head())\n",
    "print(\"\\nCorresponding CLASSIFIED_L1 values from gold_data_l23:\")\n",
    "print(gold_data_l23[gold_data_l23['CONTACTID'].isin(misclassified_df['CONTACTID'])][['CONTACTID', 'CLASSIFIED_L1']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1707181e-050e-4d6e-a557-e8bcc7ac3600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load gold dataset\n",
    "gold_data_l23 = pd.read_csv(\"/dbfs/FileStore/tables/ms/newnewbook3c.csv\")\n",
    "\n",
    "# Load all-mpnet-v2 model\n",
    "checkpoint = '/dbfs/FileStore/tables/ms/saved_mpnet_model/'\n",
    "model = SentenceTransformer(checkpoint, trust_remote_code=True)\n",
    "\n",
    "# Train SVM classifier with hyperparameter tuning\n",
    "def train_and_predict(X_train_feat, y_train, X_test_feat):\n",
    "    svm_model = SVC(probability=True)\n",
    "    svm_params = {'C': [0.1, 1], 'kernel': ['linear', 'rbf']}\n",
    "    grid_search = GridSearchCV(svm_model, svm_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train_feat, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_feat)\n",
    "    return best_model, y_pred, grid_search.best_params_\n",
    "\n",
    "# Constrain predictions based on hierarchy\n",
    "def constrained_predict(model, X, allowed_classes, label_encoder):\n",
    "    scores = model.predict_proba(X)\n",
    "    allowed_indices = [np.where(label_encoder.classes_ == c)[0][0] for c in allowed_classes if c in label_encoder.classes_]\n",
    "    preds = []\n",
    "    for row_scores in scores:\n",
    "        masked = np.full_like(row_scores, -np.inf)\n",
    "        masked[allowed_indices] = row_scores[allowed_indices]\n",
    "        pred_idx = np.argmax(masked)\n",
    "        preds.append(pred_idx)\n",
    "    return np.array(preds)\n",
    "\n",
    "# Main pipeline\n",
    "def run_hierarchical_svm_pipeline(data, level1_col, level2_col, level3_col):\n",
    "    X = data['USER_CONTENT']\n",
    "    y1 = data[level1_col]\n",
    "    y2 = data[level2_col]\n",
    "    y3 = data[level3_col]\n",
    "\n",
    "    X_train, X_test, y1_train, y1_test, y2_train, y2_test, y3_train, y3_test = train_test_split(\n",
    "        X, y1, y2, y3, test_size=0.2, random_state=42, stratify=y1\n",
    "    )\n",
    "\n",
    "    # Remove rows from the test if the labels are unseen in level 2 or 3 test sets\n",
    "    valid_l2_labels = set(y2_train)\n",
    "    valid_l3_labels = set(y3_train)\n",
    "\n",
    "    # mask for valid rows\n",
    "    valid_mask = y2_test.isin(valid_l2_labels) & y3_test.isin(valid_l3_labels)\n",
    "\n",
    "    # Apply mask\n",
    "    X_test = X_test[valid_mask]\n",
    "    y1_test = y1_test[valid_mask]\n",
    "    y2_test = y2_test[valid_mask]\n",
    "    y3_test = y3_test[valid_mask]\n",
    "\n",
    "    # Encode text\n",
    "    X_train_embedded = model.encode(X_train.tolist(), show_progress_bar=True)\n",
    "    X_test_embedded = model.encode(X_test.tolist(), show_progress_bar=True)\n",
    "\n",
    "    # Encode labels\n",
    "    le1 = LabelEncoder()\n",
    "    y1_train_enc = le1.fit_transform(y1_train)\n",
    "    y1_test_enc = le1.transform(y1_test)\n",
    "\n",
    "    le2 = LabelEncoder()\n",
    "    y2_train_enc = le2.fit_transform(y2_train)\n",
    "    y2_test_enc = le2.transform(y2_test)\n",
    "\n",
    "    le3 = LabelEncoder()\n",
    "    y3_train_enc = le3.fit_transform(y3_train)\n",
    "    y3_test_enc = le3.transform(y3_test)\n",
    "\n",
    "    # Level 1\n",
    "    model1, y1_test_pred, params1 = train_and_predict(X_train_embedded, y1_train_enc, X_test_embedded)\n",
    "\n",
    "    # Train level 2 classifier with level 1 predictions as a feature\n",
    "    X_train_level2 = np.hstack([X_train_embedded, y1_train_enc.reshape(-1,1)])\n",
    "    X_test_level2 = np.hstack([X_test_embedded, y1_test_pred.reshape(-1,1)])\n",
    "    model2, _, params2 = train_and_predict(X_train_level2, y2_train_enc, X_test_level2)\n",
    "\n",
    "    # Predict Level 2 with hierarchy constraints\n",
    "    y2_test_pred = []\n",
    "    for i in range(len(X_test_level2)):\n",
    "        pred_l1_label = le1.inverse_transform([y1_test_pred[i]])[0]\n",
    "        valid_l2 = list(hierarchy.get(pred_l1_label, {}).keys())\n",
    "        pred_l2_idx = constrained_predict(model2, X_test_level2[i:i+1], valid_l2, le2)[0]\n",
    "        y2_test_pred.append(pred_l2_idx)\n",
    "    y2_test_pred = np.array(y2_test_pred)\n",
    "\n",
    "    # Train level 3 classifier with level 2 predictions as a feature\n",
    "    print(\"Training Level 3 classifier with Level 2 predictions as feature...\")\n",
    "    X_train_level3 = np.hstack([X_train_embedded, y2_train_enc.reshape(-1,1)])\n",
    "    X_test_level3 = np.hstack([X_test_embedded, y2_test_pred.reshape(-1,1)])\n",
    "    model3, _, params3 = train_and_predict(X_train_level3, y3_train_enc, X_test_level3)\n",
    "\n",
    "    # Predict Level 3 with hierarchy constraints\n",
    "    y3_test_pred = []\n",
    "    for i in range(len(X_test_level3)):\n",
    "        pred_l1_label = le1.inverse_transform([y1_test_pred[i]])[0]\n",
    "        pred_l2_label = le2.inverse_transform([y2_test_pred[i]])[0]\n",
    "        valid_l3 = hierarchy.get(pred_l1_label, {}).get(pred_l2_label, [])\n",
    "        pred_l3_idx = constrained_predict(model3, X_test_level3[i:i+1], valid_l3, le3)[0]\n",
    "        y3_test_pred.append(pred_l3_idx)\n",
    "    y3_test_pred = np.array(y3_test_pred)\n",
    "\n",
    "    # Print accuracies\n",
    "    print(\"\\nLevel 1 Accuracy:\", accuracy_score(y1_test_enc, y1_test_pred))\n",
    "    print(classification_report(\n",
    "        y1_test_enc, y1_test_pred,\n",
    "        labels=np.unique(y1_test_enc),\n",
    "        target_names=le1.inverse_transform(np.unique(y1_test_enc)),\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"\\nLevel 2 Accuracy:\", accuracy_score(y2_test_enc, y2_test_pred))\n",
    "    print(classification_report(\n",
    "        y2_test_enc, y2_test_pred,\n",
    "        labels=np.unique(y2_test_enc),\n",
    "        target_names=le2.inverse_transform(np.unique(y2_test_enc)),\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    print(\"\\nLevel 3 Accuracy:\", accuracy_score(y3_test_enc, y3_test_pred))\n",
    "    print(classification_report(\n",
    "        y3_test_enc, y3_test_pred,\n",
    "        labels=np.unique(y3_test_enc),\n",
    "        target_names=le3.inverse_transform(np.unique(y3_test_enc)),\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    # Compute confidence scores\n",
    "    level1_confidences = model1.predict_proba(X_test_embedded)[np.arange(len(y1_test_pred)), y1_test_pred]\n",
    "    level2_confidences = model2.predict_proba(X_test_level2)[np.arange(len(y2_test_pred)), y2_test_pred]\n",
    "    level3_confidences = model3.predict_proba(X_test_level3)[np.arange(len(y3_test_pred)), y3_test_pred]\n",
    "\n",
    "    # Compute accuracies\n",
    "    acc_l1 = accuracy_score(y1_test_enc, y1_test_pred)\n",
    "    acc_l2 = accuracy_score(y2_test_enc, y2_test_pred)\n",
    "    acc_l3 = accuracy_score(y3_test_enc, y3_test_pred)\n",
    "\n",
    "    # Return everything in one dictionary\n",
    "    return {\n",
    "        'level1': (model1, y1_test_pred, params1, le1),\n",
    "        'level2': (model2, y2_test_pred, params2, le2),\n",
    "        'level3': (model3, y3_test_pred, params3, le3),\n",
    "        'X_test': X_test.reset_index(drop=True),\n",
    "        'contact_ids': data.loc[X_test.index, 'CONTACTID'].reset_index(drop=True),\n",
    "        'confidence': {\n",
    "            'level1': level1_confidences,\n",
    "            'level2': level2_confidences,\n",
    "            'level3': level3_confidences\n",
    "        },\n",
    "        'accuracy': {\n",
    "            'level1': acc_l1,\n",
    "            'level2': acc_l2,\n",
    "            'level3': acc_l3\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the pipeline\n",
    "models = run_hierarchical_svm_pipeline(gold_data_l23, 'CLASSIFIED_L1', 'CLASSIFIED_L2', 'CLASSIFIED_L3')\n",
    "\n",
    "# Get accuracies\n",
    "accuracies = models['accuracy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b445b6-9360-4c5d-a323-156572c4dd87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract necessary components from the pipeline results\n",
    "y1_test_pred = models['level1'][1]  # Predicted labels for Level 1\n",
    "le1 = models['level1'][3]  # LabelEncoder for Level 1\n",
    "X_test = models['X_test']  # Test set features\n",
    "contact_ids = models['contact_ids']  # Contact IDs for test set\n",
    "y1_test = gold_data_l23.loc[X_test.index, 'CLASSIFIED_L1'].reset_index(drop=True)  # True Level 1 labels\n",
    "\n",
    "# Create a mask for misclassified instances\n",
    "misclassified_mask = y1_test_pred != le1.transform(y1_test)\n",
    "\n",
    "# Create DataFrame for misclassified instances\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'CONTACTID': contact_ids[misclassified_mask],\n",
    "    'USER_CONTENT': X_test[misclassified_mask],\n",
    "    'True_Label': y1_test[misclassified_mask],\n",
    "    'Predicted_Label': le1.inverse_transform(y1_test_pred[misclassified_mask]),\n",
    "    'Confidence': models['confidence']['level1'][misclassified_mask]\n",
    "})\n",
    "\n",
    "# Reset index for clarity\n",
    "misclassified_df = misclassified_df.reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Misclassified Instances for Level 1:\")\n",
    "print(misclassified_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2915dab9-9410-4f70-b981-535851c04c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "misclassified_df.to_csv('misclassified_level11.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b64774d-8baf-4c5a-9e58-443e60d0b69f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract necessary components from the pipeline results\n",
    "y1_test_pred = models['level1'][1]  # Predicted labels for Level 1\n",
    "le1 = models['level1'][3]  # LabelEncoder for Level 1\n",
    "X_test = models['X_test']  # Test set features\n",
    "contact_ids = models['contact_ids']  # Contact IDs for test set\n",
    "\n",
    "# Extract true Level 1 labels directly from gold_data_l23 using the test set indices\n",
    "# Ensure indices align with X_test\n",
    "test_indices = X_test.index\n",
    "y1_test = gold_data_l23.loc[test_indices, 'CLASSIFIED_L1']\n",
    "\n",
    "# Verify that the indices match\n",
    "assert len(y1_test) == len(X_test), \"Mismatch in number of test samples\"\n",
    "assert y1_test.index.equals(X_test.index), \"Index mismatch between y1_test and X_test\"\n",
    "\n",
    "# Create a mask for misclassified instances\n",
    "# Encode y1_test using le1 for comparison with y1_test_pred\n",
    "y1_test_enc = le1.transform(y1_test)\n",
    "misclassified_mask = y1_test_pred != y1_test_enc\n",
    "\n",
    "# Create DataFrame for misclassified instances\n",
    "misclassified_df = pd.DataFrame({\n",
    "    'CONTACTID': contact_ids[misclassified_mask],\n",
    "    'USER_CONTENT': X_test[misclassified_mask],\n",
    "    'True_Label': y1_test[misclassified_mask],  # Use original CLASSIFIED_L1 values\n",
    "    'Predicted_Label': le1.inverse_transform(y1_test_pred[misclassified_mask]),\n",
    "    'Confidence': models['confidence']['level1'][misclassified_mask]\n",
    "})\n",
    "\n",
    "# Reset index for clarity\n",
    "misclassified_df = misclassified_df.reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "misclassified_df.to_csv('/dbfs/FileStore/tables/ms/misclassified_level1.csv', index=False)\n",
    "\n",
    "# Print the DataFrame to inspect\n",
    "print(\"Misclassified Instances for Level 1:\")\n",
    "print(misclassified_df)\n",
    "\n",
    "# Verify True_Label matches gold_data_l23\n",
    "print(\"\\nSample of True_Label values from misclassified_df:\")\n",
    "print(misclassified_df['True_Label'].head())\n",
    "print(\"\\nSample of CLASSIFIED_L1 values from gold_data_l23 for comparison:\")\n",
    "print(gold_data_l23.loc[test_indices[misclassified_mask], 'CLASSIFIED_L1'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2a960cd-2ddc-4025-a3eb-91bc7dca72f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check for NaNs in your embedded test features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Example for level 3 inputs (the last model in the pipeline)\n",
    "nan_rows = np.isnan(X_test_level3).any(axis=1)\n",
    "\n",
    "# See which rows are affected\n",
    "print(\"Rows with NaNs in X_test_level3:\")\n",
    "print(np.where(nan_rows)[0])\n",
    "\n",
    "# Optionally view full data row info (e.g. content and contact ID)\n",
    "print(\"Problematic rows with CONTACTID:\")\n",
    "print(models['contact_ids'][nan_rows])\n",
    "print(models['X_test'][nan_rows])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b6c4fa-35d3-4d97-8b02-891e80f5b425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### s009 Evaluate and export the supervised model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b53be10-2acf-49b0-b9b5-7c9393b1996d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Decode predictions\n",
    "pred_l1 = models['level1'][3].inverse_transform(models['level1'][1])\n",
    "pred_l2 = models['level2'][3].inverse_transform(models['level2'][1])\n",
    "pred_l3 = models['level3'][3].inverse_transform(models['level3'][1])\n",
    "\n",
    "# Get confidence scores\n",
    "conf_l1 = models['confidence']['level1']\n",
    "conf_l2 = models['confidence']['level2']\n",
    "conf_l3 = models['confidence']['level3']\n",
    "\n",
    "# Align content with test indices\n",
    "content_test = merged_df.loc[models['contact_ids'].index, 'CONTENT'].reset_index(drop=True)\n",
    "\n",
    "# Build result dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'CONTACTID': models['contact_ids'],\n",
    "    'CONTENT': content_test,\n",
    "    'PREDICTED_L1': pred_l1,\n",
    "    'CONFIDENCE_L1': conf_l1,\n",
    "    'PREDICTED_L2': pred_l2,\n",
    "    'CONFIDENCE_L2': conf_l2,\n",
    "    'PREDICTED_L3': pred_l3,\n",
    "    'CONFIDENCE_L3': conf_l3\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('hierarchical_predictions2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae568007-77bc-4819-ac2d-734c3eceb0a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upload results to Snowflake table\n",
    "df_spark = spark.createDataFrame(results_df)\n",
    "\n",
    "# Add the current timestamp as a new column\n",
    "df_spark = df_spark.withColumn(\"created_datetime\", lit(datetime.now()))\n",
    "\n",
    "# Now write to Snowflake\n",
    "df_spark.write \\\n",
    "    .format(\"snowflake\") \\\n",
    "    .options(**sf_options) \\\n",
    "    .option(\"dbtable\", \"SANDBOX.AIC_LABEL_LEVEL123_20250612\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c7e746-d10a-4f52-af9d-234c37a586ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Plot accuracy by topic level\n",
    "acc_df = pd.DataFrame({\n",
    "    'Level': ['Level 1', 'Level 2', 'Level 3'],\n",
    "    'Accuracy': [accuracies['level1'], accuracies['level2'], accuracies['level3']]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "barplot = sns.barplot(data=acc_df, x='Level', y='Accuracy', palette='Blues_d')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Hierarchical SVM Accuracy by Level')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Classification Level')\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add accuracy values on top of the bars\n",
    "for i, acc in enumerate(acc_df['Accuracy']):\n",
    "    plt.text(i, acc + 0.02, f\"{acc:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) MS-AIC Chat Topic Modelling v2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
